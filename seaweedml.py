# -*- coding: utf-8 -*-
"""SeaWeedML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1viPcx1ejVhh0l3YVM_ipGU5IMxUHSqBt

**IMPORT DATA**
"""

#Ambil data dari google drive
from google.colab import drive
import pandas as pd

# Mount Google Drive
drive.mount('/content/drive')

# Path to your Excel file in Google Drive
file_path = '/content/drive/My Drive/DATA RUMPUT LAUT.xlsx'  # Replace with your file path

# Read the Excel file into a pandas DataFrame
data = pd.read_excel(file_path, sheet_name='data')

# Now you can work with the 'data' DataFrame
# For example, to display the first 5 rows:
data.head()

"""**DATA UNDERSTANDING**"""

#Data Understanding
import seaborn as sns
import matplotlib.pyplot as plt

# Informasi struktur data
print("\nDataset Info:")
print(data.info())

# Statistik deskriptif
print("\nDescriptive Statistics:")
print(data.describe())

# Distribusi Data
# Histogram untuk setiap kanal
data.hist(bins=20, figsize=(16, 12), color='steelblue', edgecolor='black')
plt.suptitle("Distribusi Data pada Setiap Fitur")
plt.show()

# Analisis Korelasi
# Matriks korelasi
corr_matrix = data.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title("Heatmap Korelasi")
plt.show()

# Deteksi Outliers
# Boxplot untuk masing-masing kanal
plt.figure(figsize=(16, 8))
sns.boxplot(data=data, orient="h", palette="Set2")
plt.title("Deteksi Outliers pada Setiap Fitur")
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import linregress

# Scatter Plot untuk Hubungan Antar Kanal dan Kadar Air
plt.figure(figsize=(20, 20))  # Atur ukuran plot untuk semua kanal
plt.suptitle("Scatter Plot: Hubungan Setiap Kanal dan Kadar Air dengan R²", fontsize=20)

# Iterasi untuk setiap kanal (asumsi nama kolom Kanal_A, Kanal_B, ..., Kanal_L)
channels = ['Kanal A', 'Kanal B', 'Kanal C', 'Kanal D', 'Kanal E', 'Kanal F',
            'Kanal G', 'Kanal H', 'Kanal R', 'Kanal I', 'Kanal S', 'Kanal J',
            'Kanal T', 'Kanal U', 'Kanal V', 'Kanal W', 'Kanal K', 'Kanal L']

kanal = ['CH1','CH2','CH3','CH4','CH5','CH6','CH7','CH8','CH9','CH10','CH11','CH12','CH13',
         'CH14','CH15','CH16','CH17','CH18']

for i, (kanal_name, channel) in enumerate(zip(kanal, channels), 1):  # Iterasi menggunakan kanal untuk data
    plt.subplot(5, 4, i)  # Atur subplot grid (5 baris x 4 kolom untuk 18 kanal)

    # Ambil data kanal dan kadar air
    x = data[kanal_name]
    y = data['Kadar Air']

    # Lakukan regresi linier
    slope, intercept, r_value, p_value, std_err = linregress(x, y)

    # Hitung R²
    r_squared = r_value**2

    # Buat scatter plot
    plt.scatter(x, y, alpha=0.6, color='steelblue')

    # Tambahkan garis regresi
    plt.plot(x, slope*x + intercept, color='red', linestyle='--')

    # Tampilkan R² di plot
    plt.title(f"{channel} vs Kadar Air\nR² = {r_squared:.4f}")
    plt.xlabel(kanal_name)  # Label X sesuai nama kanal
    plt.ylabel("Kadar Air")  # Label Y tetap "Kadar Air"

plt.tight_layout(rect=[0, 0, 1, 0.95])  # Supaya tidak tumpang tindih
plt.show()

# Analisis Target
# Histogram target (misal: 'Kadar_Air')
plt.figure(figsize=(8, 6))
sns.histplot(data['Kadar Air'], kde=True, color='blue', bins=20)
plt.title("Distribusi Target: Kadar Air")
plt.xlabel("Kadar Air")
plt.ylabel("Frekuensi")
plt.show()

"""**DATA PREPARATION**

1. Standarisasi
2. PCA
3. Train-Test Split
"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Standarisasi data
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data.drop(columns=['Kadar Air']))  # Hapus 'Kadar Air' dari data fitur

# Inisialisasi PCA dan fit pada data
pca = PCA()
pca.fit(data_scaled)

# Menyusun data explained variance dan cumulative variance
explained_variance = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance)

# Membuat DataFrame untuk menampilkan dalam bentuk tabel
variance_df = pd.DataFrame({
    'Explained Variance': explained_variance,
    'Cumulative Variance': cumulative_variance
}, index=[f'Komponen {i+1}' for i in range(len(explained_variance))])

# Menampilkan tabel
print(variance_df)

# Plot explained variance ratio
plt.figure(figsize=(8, 6))
plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)
plt.xlabel('PCA Komponen')
plt.ylabel('Varians yang Dijelaskan')
plt.title('Explained Variance untuk Setiap Komponen PCA')
plt.xticks(np.arange(1, len(pca.explained_variance_ratio_) + 1, 1))
plt.show()

# Menampilkan total explained variance kumulatif
explained_variance_cumulative = np.cumsum(pca.explained_variance_ratio_)
plt.figure(figsize=(8, 6))
plt.plot(range(1, len(explained_variance_cumulative) + 1), explained_variance_cumulative, marker='o', color='b')
plt.xlabel('Jumlah Komponen PCA')
plt.ylabel('Kumulatif Varians yang Dijelaskan')
plt.title('Kumulatif Varians yang Dijelaskan oleh PCA')
plt.xticks(np.arange(1, len(pca.explained_variance_ratio_) + 1, 1))
plt.grid(True)
plt.show()

# Menampilkan koefisien loading untuk setiap komponen
pca_components = pca.components_

# Membuat DataFrame untuk memudahkan interpretasi
loading_df = pd.DataFrame(pca_components, columns=data.drop(columns=['Kadar Air']).columns)

# Menampilkan loading untuk 6 komponen pertama
print("Loading untuk 6 komponen pertama:")
print(loading_df.iloc[:6, :])

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Standarisasi data
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data.drop(columns=['Kadar Air']))  # Hapus 'Kadar Air' dari data fitur

# Inisialisasi PCA dan pilih 6 komponen utama
pca = PCA(n_components=6)
pca.fit(data_scaled)

# Mengambil komponen utama (koefisien) untuk 6 komponen pertama
pca_components = pca.components_

# Visualisasikan komponen PCA dengan heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(pca_components, annot=True, cmap='coolwarm', xticklabels=data.drop(columns=['Kadar Air']).columns,
            yticklabels=[f'PC{i+1}' for i in range(6)], cbar_kws={'label': 'Kontribusi Komponen'}, fmt='.2f')

plt.title('Heatmap Kontribusi Setiap Kanal terhadap 6 Komponen PCA')
plt.xlabel('Kanal')
plt.ylabel('Komponen PCA')
plt.show()

# Menampilkan variance yang dijelaskan oleh masing-masing komponen
explained_variance = pca.explained_variance_ratio_
print(f"Explained Variance per Komponen: {explained_variance}")

from sklearn.model_selection import train_test_split

# Misalkan data_pca adalah data yang telah melalui PCA (X)
# Misalkan target adalah kadar air (Y)
X = data_pca  # Data komponen PCA
Y = data['Kadar Air']  # Target kadar air

# Melakukan train-test split dengan proporsi 80% untuk training dan 20% untuk testing
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Menampilkan ukuran data setelah split
print(f'Ukuran data pelatihan X: {X_train.shape}')
print(f'Ukuran data pengujian X: {X_test.shape}')
print(f'Ukuran data pelatihan Y: {Y_train.shape}')
print(f'Ukuran data pengujian Y: {Y_test.shape}')

from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input


# 1. RANDOM FOREST
print("RANDOM FOREST")
# Inisialisasi model Random Forest
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Melatih model dengan data pelatihan
rf_model.fit(X_train, Y_train)

# 2. MLP (multi-layer perceptron)
print("MLP")
# Inisialisasi model Neural Network (MLP)
nn_model = MLPRegressor(hidden_layer_sizes=(50, 30), max_iter=1000, random_state=42)

# Melatih model dengan data pelatihan
nn_model.fit(X_train, Y_train)

# 3. KNN
print("KNN")
# Inisialisasi model KNN
knn_model = KNeighborsRegressor(n_neighbors=5)

# Melatih model dengan data pelatihan
knn_model.fit(X_train, Y_train)

# 4. ADABOOST
print("ADABOOST")
# Inisialisasi model AdaBoost
ada_model = AdaBoostRegressor(n_estimators=50, random_state=42)

# Melatih model dengan data pelatihan
ada_model.fit(X_train, Y_train)

# 5. GRADIENT BOOST
print("GRADIENT BOOST")
# Inisialisasi model Gradient Boosting
gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)

# Melatih model dengan data pelatihan
gb_model.fit(X_train, Y_train)

# 6. Model Deep Learning Neural Network
print("DNN")
# Inisialisasi model Deep Learning (DNN)
model_nn = Sequential()
model_nn.add(Input(shape=(6,)))  # Sesuaikan input dengan jumlah fitur yang benar
model_nn.add(Dense(64, input_dim=18, activation='relu'))  # Layer pertama dengan 64 neuron
model_nn.add(Dense(32, activation='relu'))  # Layer kedua dengan 32 neuron
model_nn.add(Dense(1))  # Output layer untuk regresi

# Kompilasi model
model_nn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])

# Melatih model dengan data pelatihan
history_nn = model_nn.fit(X_train, Y_train, epochs=100, batch_size=32, validation_data=(X_test, Y_test), verbose=1)

# 7. Model Decision Tree
print("DECISION TREE")
# Inisialisasi model DecisionTreeRegressor
dt_model = DecisionTreeRegressor(random_state=42)

# Melatih model dengan data pelatihan
dt_model.fit(X_train, Y_train)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Prediksi menggunakan data pelatihan dan data pengujian
Y_pred_train_rf = rf_model.predict(X_train)
Y_pred_test_rf = rf_model.predict(X_test)

Y_pred_train_nn = nn_model.predict(X_train)
Y_pred_test_nn = nn_model.predict(X_test)

Y_pred_train_knn = knn_model.predict(X_train)
Y_pred_test_knn = knn_model.predict(X_test)

Y_pred_train_ada = ada_model.predict(X_train)
Y_pred_test_ada = ada_model.predict(X_test)

Y_pred_train_gb = gb_model.predict(X_train)
Y_pred_test_gb = gb_model.predict(X_test)

Y_pred_train_dnn = model_nn.predict(X_train)
Y_pred_test_dnn = model_nn.predict(X_test)

Y_pred_train_dt = dt_model.predict(X_train)
Y_pred_test_dt = dt_model.predict(X_test)

# Menghitung MSE, MAE, RMSE, dan R^2 untuk Random Forest (training & testing)
mse_train_rf = mean_squared_error(Y_train, Y_pred_train_rf)
mae_train_rf = mean_absolute_error(Y_train, Y_pred_train_rf)
rmse_train_rf = np.sqrt(mse_train_rf)
r2_train_rf = r2_score(Y_train, Y_pred_train_rf)

mse_test_rf = mean_squared_error(Y_test, Y_pred_test_rf)
mae_test_rf = mean_absolute_error(Y_test, Y_pred_test_rf)
rmse_test_rf = np.sqrt(mse_test_rf)
r2_test_rf = r2_score(Y_test, Y_pred_test_rf)

# Menghitung MSE, MAE, RMSE, dan R^2 untuk Neural Network (training & testing)
mse_train_nn = mean_squared_error(Y_train, Y_pred_train_nn)
mae_train_nn = mean_absolute_error(Y_train, Y_pred_train_nn)
rmse_train_nn = np.sqrt(mse_train_nn)
r2_train_nn = r2_score(Y_train, Y_pred_train_nn)

mse_test_nn = mean_squared_error(Y_test, Y_pred_test_nn)
mae_test_nn = mean_absolute_error(Y_test, Y_pred_test_nn)
rmse_test_nn = np.sqrt(mse_test_nn)
r2_test_nn = r2_score(Y_test, Y_pred_test_nn)

# Menghitung MSE, MAE, RMSE, dan R^2 untuk KNN (training & testing)
mse_train_knn = mean_squared_error(Y_train, Y_pred_train_knn)
mae_train_knn = mean_absolute_error(Y_train, Y_pred_train_knn)
rmse_train_knn = np.sqrt(mse_train_knn)
r2_train_knn = r2_score(Y_train, Y_pred_train_knn)

mse_test_knn = mean_squared_error(Y_test, Y_pred_test_knn)
mae_test_knn = mean_absolute_error(Y_test, Y_pred_test_knn)
rmse_test_knn = np.sqrt(mse_test_knn)
r2_test_knn = r2_score(Y_test, Y_pred_test_knn)

# Menghitung MSE, MAE, RMSE, dan R^2 untuk ADABOOST (training & testing)
mse_train_ada = mean_squared_error(Y_train, Y_pred_train_ada)
mae_train_ada = mean_absolute_error(Y_train, Y_pred_train_ada)
rmse_train_ada = np.sqrt(mse_train_ada)
r2_train_ada = r2_score(Y_train, Y_pred_train_ada)

mse_test_ada = mean_squared_error(Y_test, Y_pred_test_ada)
mae_test_ada = mean_absolute_error(Y_test, Y_pred_test_ada)
rmse_test_ada = np.sqrt(mse_test_ada)
r2_test_ada = r2_score(Y_test, Y_pred_test_ada)

# Menghitung MSE, MAE, RMSE, dan R^2 untuk Gradient Boost (training & testing)
mse_train_gb = mean_squared_error(Y_train, Y_pred_train_gb)
mae_train_gb = mean_absolute_error(Y_train, Y_pred_train_gb)
rmse_train_gb = np.sqrt(mse_train_gb)
r2_train_gb = r2_score(Y_train, Y_pred_train_gb)

mse_test_gb = mean_squared_error(Y_test, Y_pred_test_gb)
mae_test_gb = mean_absolute_error(Y_test, Y_pred_test_gb)
rmse_test_gb = np.sqrt(mse_test_gb)
r2_test_gb = r2_score(Y_test, Y_pred_test_gb)

# Menghitung MSE, MAE, RMSE, dan R^2 untuk Deep Neural Network (training & testing)
mse_train_dnn = mean_squared_error(Y_train, Y_pred_train_dnn)
mae_train_dnn = mean_absolute_error(Y_train, Y_pred_train_dnn)
rmse_train_dnn = np.sqrt(mse_train_dnn)
r2_train_dnn = r2_score(Y_train, Y_pred_train_dnn)

mse_test_dnn = mean_squared_error(Y_test, Y_pred_test_dnn)
mae_test_dnn = mean_absolute_error(Y_test, Y_pred_test_dnn)
rmse_test_dnn = np.sqrt(mse_test_dnn)
r2_test_dnn = r2_score(Y_test, Y_pred_test_dnn)

# Menghitung MSE, MAE, RMSE, dan R^2 untuk Decision Tree (training & testing)
mse_train_dt = mean_squared_error(Y_train, Y_pred_train_dt)
mae_train_dt = mean_absolute_error(Y_train, Y_pred_train_dt)
rmse_train_dt = np.sqrt(mse_train_dt)
r2_train_dt = r2_score(Y_train, Y_pred_train_dt)

mse_test_dt = mean_squared_error(Y_test, Y_pred_test_dt)
mae_test_dt = mean_absolute_error(Y_test, Y_pred_test_dt)
rmse_test_dt = np.sqrt(mse_test_dt)
r2_test_dt = r2_score(Y_test, Y_pred_test_dt)

# Tampilkan evaluasi
print("MODEL EVALUATION")
models = ['Random Forest', 'Neural Network', 'KNN', 'Adaboost', 'Gradient Boost', 'Deep Neural Network', 'Decision Tree']
metrics = ['MSE (Train)', 'MAE (Train)', 'RMSE (Train)', 'R² (Train)', 'MSE (Test)', 'MAE (Test)', 'RMSE (Test)', 'R² (Test)']

# Hasil evaluasi
results = [
    [mse_train_rf, mae_train_rf, rmse_train_rf, r2_train_rf, mse_test_rf, mae_test_rf, rmse_test_rf, r2_test_rf],
    [mse_train_nn, mae_train_nn, rmse_train_nn, r2_train_nn, mse_test_nn, mae_test_nn, rmse_test_nn, r2_test_nn],
    [mse_train_knn, mae_train_knn, rmse_train_knn, r2_train_knn, mse_test_knn, mae_test_knn, rmse_test_knn, r2_test_knn],
    [mse_train_ada, mae_train_ada, rmse_train_ada, r2_train_ada, mse_test_ada, mae_test_ada, rmse_test_ada, r2_test_ada],
    [mse_train_gb, mae_train_gb, rmse_train_gb, r2_train_gb, mse_test_gb, mae_test_gb, rmse_test_gb, r2_test_gb],
    [mse_train_dnn, mae_train_dnn, rmse_train_dnn, r2_train_dnn, mse_test_dnn, mae_test_dnn, rmse_test_dnn, r2_test_dnn],
    [mse_train_dt, mae_train_dt, rmse_train_dt, r2_train_dt, mse_test_dt, mae_test_dt, rmse_test_dt, r2_test_dt]
]

# Print as table
eval_df = pd.DataFrame(results, columns=metrics, index=models)
print(eval_df)

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Misalkan eval_df adalah DataFrame yang berisi hasil evaluasi model yang telah dihitung sebelumnya
# Anda perlu memastikan bahwa hasil evaluasi untuk train dan test sudah ada di eval_df

# Membuat plotting untuk MAE, MSE, RMSE, dan R²

metrics = ['MAE', 'MSE', 'RMSE', 'R²']
models = eval_df.index  # Model names
train_metrics = ['MAE (Train)', 'MSE (Train)', 'RMSE (Train)', 'R² (Train)']
test_metrics = ['MAE (Test)', 'MSE (Test)', 'RMSE (Test)', 'R² (Test)']

# Set plot size
fig, axes = plt.subplots(4, 1, figsize=(12, 10))

# Plot untuk MAE
axes[0].bar(models, eval_df[train_metrics[0]], width=0.3, label='Train', color='c', align='center')
axes[0].bar(models, eval_df[test_metrics[0]], width=0.3, label='Test', color='m', align='edge')
axes[0].set_title('MAE Comparison')
axes[0].set_ylabel('MAE')
axes[0].legend()

# Plot untuk MSE
axes[1].bar(models, eval_df[train_metrics[1]], width=0.3, label='Train', color='c', align='center')
axes[1].bar(models, eval_df[test_metrics[1]], width=0.3, label='Test', color='m', align='edge')
axes[1].set_title('MSE Comparison')
axes[1].set_ylabel('MSE')
axes[1].legend()

# Plot untuk RMSE
axes[2].bar(models, eval_df[train_metrics[2]], width=0.3, label='Train', color='c', align='center')
axes[2].bar(models, eval_df[test_metrics[2]], width=0.3, label='Test', color='m', align='edge')
axes[2].set_title('RMSE Comparison')
axes[2].set_ylabel('RMSE')
axes[2].legend()

# Plot untuk R²
axes[3].bar(models, eval_df[train_metrics[3]], width=0.3, label='Train', color='c', align='center')
axes[3].bar(models, eval_df[test_metrics[3]], width=0.3, label='Test', color='m', align='edge')
axes[3].set_title('R² Comparison')
axes[3].set_ylabel('R²')
axes[3].legend()

# Adjust layout and show plot
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error

# Ambil 10 data acak dari data training (X_train dan Y_train) menggunakan numpy
random_indices = np.random.choice(len(X_train), size=10, replace=False)

# Memastikan pengambilan data untuk keduanya X_train dan Y_train menggunakan iloc atau array indexing
X_random = X_train[random_indices]
Y_random = Y_train.iloc[random_indices]  # Pastikan menggunakan iloc untuk Series (Y_train)

# Prediksi kadar air menggunakan model-model yang telah dibangun
Y_pred_rf = rf_model.predict(X_random)
Y_pred_nn = nn_model.predict(X_random)
Y_pred_knn = knn_model.predict(X_random)
Y_pred_ada = ada_model.predict(X_random)
Y_pred_gb = gb_model.predict(X_random)
Y_pred_dnn = model_nn.predict(X_random)
Y_pred_dt = dt_model.predict(X_random)

# Menghitung error (MAE) untuk setiap model
mae_rf = mean_absolute_error(Y_random, Y_pred_rf)
mae_nn = mean_absolute_error(Y_random, Y_pred_nn)
mae_knn = mean_absolute_error(Y_random, Y_pred_knn)
mae_ada = mean_absolute_error(Y_random, Y_pred_ada)
mae_gb = mean_absolute_error(Y_random, Y_pred_gb)
mae_dnn = mean_absolute_error(Y_random, Y_pred_dnn)
mae_dt = mean_absolute_error(Y_random, Y_pred_dt)

# Menyusun hasil prediksi dalam dictionary (memastikan format 1 dimensi untuk setiap model)
results = {
    'Actual Value': np.round(Y_random.values, 2),  # Round to 2 decimal places
    'RF Prediction': np.round(Y_pred_rf.flatten(), 2),  # Round to 2 decimal places
    'NN Prediction': np.round(Y_pred_nn.flatten(), 2),  # Round to 2 decimal places
    'KNN Prediction': np.round(Y_pred_knn.flatten(), 2),  # Round to 2 decimal places
    'Adaboost Prediction': np.round(Y_pred_ada.flatten(), 2),  # Round to 2 decimal places
    'GB Prediction': np.round(Y_pred_gb.flatten(), 2),  # Round to 2 decimal places
    'DNN Prediction': np.round(Y_pred_dnn.flatten(), 2),  # Round to 2 decimal places
    'DT Prediction': np.round(Y_pred_dt.flatten(), 2)   # Round to 2 decimal places
}

# Convert to DataFrame
eval_df = pd.DataFrame(results)

# Menambahkan kolom MAE per model ke DataFrame dan membulatkan nilai MAE ke 2 angka desimal
eval_df['RF Error (MAE)'] = np.round(mae_rf, 2)
eval_df['NN Error (MAE)'] = np.round(mae_nn, 2)
eval_df['KNN Error (MAE)'] = np.round(mae_knn, 2)
eval_df['Adaboost Error (MAE)'] = np.round(mae_ada, 2)
eval_df['GB Error (MAE)'] = np.round(mae_gb, 2)
eval_df['DNN Error (MAE)'] = np.round(mae_dnn, 2)
eval_df['DT Error (MAE)'] = np.round(mae_dt, 2)

# Menampilkan DataFrame hasil evaluasi
print(eval_df)