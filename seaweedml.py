# -*- coding: utf-8 -*-
"""SeaWeedML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1viPcx1ejVhh0l3YVM_ipGU5IMxUHSqBt

# Proyek Machine Learning Kadar Air Rumput Laut Kappaphycus Alvarezii - Hollanda Arief Kusuma

## Domain Proyek
_Kappaphycus alvarezii_ (_K. alvarezii_), juga dikenal sebagai _Eucheuma cottonii_, adalah spesies rumput laut merah penting secara komersial yang ditemukan di perairan tropis [1]. Rumput laut ini dijumpai di Kepulauan Riau terutama di Kabupaten Karimun, Kabupaten Natuna, dan Kabupaten Lingga [2]–[4]. Rumput laut ini memiliki potensi ekonomi yang besar sebagai sumber bahan baku dalam industri pangan, farmasi, dan kosmetik [5]. Pertumbuhan dan kesehatan rumput laut ini sangat dipengaruhi oleh faktor lingkungan, terutama ketersediaan air dan kandungan air dalam jaringannya [6].
Pemantauan kandungan air pada _K. alvarezii_ sangat penting baik untuk pertumbuhan dan kesehatannya, serta untuk aplikasi industri. Dari sudut pandang pertumbuhan dan kesehatan, kandungan air mempengaruhi proses fisiologis rumput laut, seperti fotosintesis dan respirasi, yang penting untuk pertumbuhan dan kelangsungan hidupnya [7], [8]. Selain itu, kandungan air dapat mempengaruhi terjadinya epifit dan infeksi penyakit, sehingga dapat berdampak negatif terhadap pertumbuhan _K. alvarezii_ [9]. Dari perspektif aplikasi industri, kadar air merupakan faktor kunci dalam menentukan kualitas dan hasil karagenan, hidrokoloid yang diekstraksi dari _K. alvarezii_, yang digunakan dalam berbagai produk makanan dan non-makanan [10].
Metode konvensional dalam mengukur kadar air dalam rumput laut menghadapi beberapa tantangan, termasuk proses yang memakan waktu, tenaga, dan invasif [11]. Oleh karena itu, pengembangan metode non-destruktif dan cepat untuk karakterisasi kandungan air dalam rumput laut menjadi perhatian utama. Salah satunya menggunakan metode spektrofotometri. Sensor spektroskopi AS7265x beroperasi dengan mengukur spektrum cahaya dalam rentang UV, sinar tampak, dan IR, memberikan solusi portabel dan efisien untuk analisis kualitatif dan kuantitatif [12]. Sensor ini memiliki resolusi 25-50 nm dengan rentang pengukuran dari 410 nm hingga 940 nm [13]. Sensor ini menawarkan keunggulan dibandingkan metode konvensional, seperti sensor spektro [14] dan spektrograf tradisional [15], karena ukurannya yang ringkas dan kemampuannya mengukur rentang panjang gelombang yang luas.
Sensor AS7265x perlu diintegrasikan dengan mikrokontroler untuk memperoleh data secara akurat dan efisien. Perangkat akuisisi data merupakan integrasi antara sensor, mikrokontroler, micro SD, Real Time Clock, display, dan komponen lainnya yang akan membuat sebuah perangkat akuisisi data yang dapat melakukan pengambilan, penyimpanan, dan tampilan data secara efektif dan efisien. Mikrokontroler berperan sebagai otak sistem yang mengatur operasi dan interaksi antara semua komponen tersebut. Beberapa penelitian menggunakan Arduino [16]–[21] dan ESP32 [21]–[23]. Namun Refly dan Kusuma [24] menyampaikan bahwa ESP32 lebih rendah konsumsi dayanya daripada Arduino. Oleh karena itu, pemilihan mikrokontroler dengan konsumsi daya yang rendah, seperti ESP32, menjadi krusial untuk memastikan kinerja perangkat akuisisi data secara optimal.
Karakterisasi spektral kandungan air pada _K. alvarezii_ di wilayah Kepulauan Riau atau Indonesia pada umumnya merupakan bidang yang belum banyak diteliti secara komprehensif. Sementara penelitian telah mengevaluasi pertumbuhan dan kualitas produk _K. alvarezii_ di berbagai lokasi budidaya di Indonesia [25], menilai kesesuaian perairan untuk budidayanya di Kepulauan Obi [26], dan mengevaluasi kesesuaian air untuk keberlanjutannya. budidaya di Kalimantan Utara [27], tidak ada yang secara khusus berfokus pada karakterisasi spektral kandungan air. Kesenjangan ini memberikan peluang untuk mengeksplorasi potensi penggunaan karakterisasi spektral untuk menilai kandungan air di _K. alvarezii_. Oleh karena itu, penelitian ini bertujuan untuk mengisi kesenjangan pengetahuan tersebut dengan menggunakan sensor spektroskopi AS7265x. Dengan memanfaatkan sensor spektroskopi AS7265x dan _machine learning_, diharapkan penelitian ini dapat memberikan kontribusi dalam pemahaman yang lebih baik tentang hubungan antara karakteristik spektral dengan kandungan air dalam rumput laut _K. alvarezii_.

### Daftar Pustaka:

[1]	R. Rama, L. Ode Muhammad Aslan, W. Iba, A. R. Nurdin, A. Armin, and Y. Yusnaeni, “Seaweed Cultivation of Micropropagated Seaweed (Kappaphycus alvarezii) in Bungin Permai Coastal Waters, Tinanggea Sub-District, South Konawe Regency, South East Sulawesi.,” IOP Conf. Ser. Earth Environ. Sci., vol. 175, p. 012219, Jul. 2018, doi: 10.1088/1755-1315/175/1/012219.

[2]	A. F. Ilhamdy, Jumsurizal, W. K. Shabilla, and G. Pratama, “Sifat Fisiko-Kimia Semi Refined Carrageenan (SRC) Kappaphycus Alvarezii Dari Perairan Karimun, Kepulauan Riau, Indonesia,” J. Perikan. dan Kelaut., vol. 9, no. 1, pp. 125–136, 2019, [Online]. Available: https://jurnal.untirta.ac.id/index.php/jpk/article/view/7079

[3]	Astika, A. F. Ilhamdy, and R. M. S. Putri, “Karakterisasi Beberapa Rumput Laut Dari Perairan Natuna Sebagai Sediaan Kosmetik,” Marinade, vol. 05, no. 02, pp. 77–84, 2022, [Online]. Available: https://ojs.umrah.ac.id/index.php/marinade/article/view/4667/1852

[4]	Herianto, “Evaluasi Pertumbuhan Rumput Laut Kappaphycus Alvarezii Dengan Jarak Peletakan Wadah Berbeda Ketinggian Dari Dasar Perairan,” [Bachelor Thesis] Universitas Maritim Raja Ali Haji, 2024. [Online]. Available: http://repositori.umrah.ac.id/6906/

[5]	J. Necas and L. Bartosikova, “Carrageenan: A review,” Vet. Med. (Praha)., vol. 58, no. 4, pp. 187–205, 2013, doi: 10.17221/6758-VETMED.

[6]	A. R. Sunny, “A review on effect of global climate change on seaweed and seagrass,” Int. J. Fish. Aquat. Stud., vol. 5, no. 6, pp. 19–22, 2017, [Online]. Available: https://www.fisheriesjournal.com/archives/2017/vol5issue6/PartA/5-5-49-118.pdf

[7]	P. G. Araújo, A. E. Nardelli, V. C. Gelli, M. T. Fujii, and F. Chow, “Monitoring environmental risk of the exotic species Kappaphycus alvarezii (Rhodophyta), after two decades of introduction in southeastern Brazil,” Bot. Mar., vol. 63, no. 6, pp. 551–558, Dec. 2020, doi: 10.1515/bot-2020-0052.

[8]	B. Castelar, R. P. Reis, and M. Bastos, “Contribuição ao protocolo de monitoramento ambiental da maricultura de Kappaphycus alvarezii (Doty) Doty ex P.C. Silva (Areschougiaceae - Rhodophyta) na baía de Sepetiba, RJ, Brasil,” Acta Bot. Brasilica, vol. 23, no. 3, pp. 613–617, Sep. 2009, doi: 10.1590/S0102-33062009000300001.

[9]	M. Ateweberhan, A. Rougier, and C. Rakotomahazo, “Influence of environmental factors and farming technique on growth and health of farmed Kappaphycus alvarezii (cottonii) in south-west Madagascar,” J. Appl. Phycol., vol. 27, no. 2, pp. 923–934, Apr. 2015, doi: 10.1007/s10811-014-0378-3.

[10]	L. O. M. Aslan et al., “Field cultivation of Kappaphycus alvarezii (DOTY) doty ex silva using tissue-cultured seedlings at bungin permai costal waters, south konawe, Southeast (SE) Sulawesi: the third year of seaweed growth monitoring,” IOP Conf. Ser. Earth Environ. Sci., vol. 473, no. 1, p. 012007, Mar. 2020, doi: 10.1088/1755-1315/473/1/012007.

[11]	L. Zhang, E. Gionfriddo, V. Acquaro, and J. Pawliszyn, “Direct immersion solid-phase microextraction analysis of multi-class contaminants in edible seaweeds by gas chromatography-mass spectrometry,” Anal. Chim. Acta, vol. 1031, pp. 83–97, Nov. 2018, doi: 10.1016/j.aca.2018.05.066.

[12]	B. Daurai, S. S. Ramchiary, and M. Gogoi, “Comparison of Sparkfun TRIAD AS7265x spectroscopy sensor device with a Spectrophotometer for qualitative and quantitative analysis,” in 2023 4th International Conference on Computing and Communication Systems (I3CS), IEEE, Mar. 2023, pp. 1–3. doi: 10.1109/I3CS58314.2023.10127282.

[13]	ams AG, “AS7265x Datasheet,” Premstaetten, 2018. [Online]. Available: https://ams.com/documents/20143/36005/AS7265x_DS000612_1-00.pdf

[14]	R. Stanziola, B. Momiroff, and H. Hemmendinger, “The spectro sensor—A new generation spectrophotometer,” Color Res. Appl., vol. 4, no. 3, pp. 157–163, 1979, doi: 10.1002/col.5080040308.

[15]	J. Allington-Smith and J. Bland-Hawthorn, “Astrophotonic spectroscopy: defining the potential advantage,” Mon. Not. R. Astron. Soc., Mar. 2010, doi: 10.1111/j.1365-2966.2009.16173.x.

[16]	H. A. Kusuma, R. Anjasmara, T. Suhendra, A. H. Yunianto, and S. Nugraha, “An IoT Based Coastal Weather and Air Quality Monitoring Using GSM Technology,” J. Phys. Conf. Ser., vol. 1501, no. 012004, 2020, doi: 10.1088/1742-6596/1501/1/012004.

[17]	R. Anjasmara, T. Suhendra, and A. H. Yunianto, “Implementasi Sistem Monitoring Kecepatan Angin, Suhu, dan Kelembaban Berbasis Web di Daerah Kepulauan,” J. Appl. Electr. Eng., vol. 3, no. 2, pp. 29–35, Dec. 2019, doi: 10.30871/jaee.v3i2.1485.

[18]	H. A. Kusuma, R. Purbakawaca, I. R. Pamungkas, L. N. Fikry, and S. S. Maulizar, “Design and Implementation of IoT-Based Water Pipe Pressure Monitoring Instrument,” J. Elektron. dan Telekomun., vol. 21, no. 1, pp. 41–47, 2021, doi: 10.14203/jet.v21.41-44.

[19]	H. A. Kusuma, M. I. Wahyuni, and S. Nugraha, “Pengembangan Instrumen Pengukuran Aliran Air Berbasis Internet of Things (IoT),” J. Elektro dan Mesin Terap., vol. 7, no. 1, pp. 47–56, May 2021, doi: 10.35143/elementer.v7i1.4627.

[20]	S. Nugraha, R. T. Putra, R. Pramana, H.A. Kusuma, T. Suhendra, E. Prayetno, and D. Nusyirwan, “Monitoring Keasaman dan Kekeruhan Air menggunakan Mikrokontroler Berbasis Internet of Things,” J. Sustain. J. Has. Penelit. dan Ind. Terap., vol. 9, no. 2, pp. 60–66, Oct. 2020, doi: 10.31629/sustainable.v9i2.2765.

[21]	H. Kusuma, M. A. Akbar, T. Suhendra, A. Zuchriadi, and A. K. A. Cintra, “IoT Sea Level Monitoring Development and Field Testing Study,” ELECTRON J. Ilm. Tek. Elektro, vol. 4, no. 2, pp. 70–77, Nov. 2023, doi: 10.33019/electron.v4i2.50.

[22]	T. Akbar, H. Fakhrurroja, and H. Kusuma, “Development of Temperature Control and Monitoring System for Precision Aquaculture Based on the Internet of Things,” in Proceedings of the 1st International Conference on Sustainable Engineering Development and Technological Innovation, ICSEDTI 2022, 11-13 October 2022, Tanjungpinang, Indonesia, EAI, 2023. doi: 10.4108/eai.11-10-2022.2326275.

[23]	H. A. Kusuma, D. Oktavia, S. Nugraha, T. Suhendra, and S. Refly, “Sensor BMP280 Statistical Analysis for Barometric Pressure Acquisition,” IOP Conf. Ser. Earth Environ. Sci., vol. 1148, no. 1, pp. 0–9, 2023, doi: 10.1088/1755-1315/1148/1/012008.

[24]	S. Refly and H. A. Kusuma, “Analisis Konsumsi dan Fluktuasi Arus dan Daya pada Mikrokontroler Menggunakan Sensor INA219,” J. Sustain. J. Has. Penelit. dan Ind. Terap., vol. 11, no. 1, pp. 44–48, 2022.

[25]	N. F. Simatupang, P. R. Pong-Masak, P. Ratnawati, Agusman, N. A. Paul, and M. A. Rimmer, “Growth and product quality of the seaweed Kappaphycus alvarezii from different farming locations in Indonesia,” Aquac. Reports, vol. 20, p. 100685, Jul. 2021, doi: 10.1016/j.aqrep.2021.100685.

[26]	R. Labenua and M. Aris, “Suitability Of Kappaphycus alvarezi Cultivation In Obi Island, North Maluku,” J. Ilm. PLATAX, vol. 9, no. 2, p. 217, Aug. 2021, doi: 10.35800/jip.9.2.2021.33048.

[27]	E. Maradhy, R. S. Nazriel, S. H. Sutjahjo, M. S. Rusli, W. Widiatmaka, and M. F. A. Sondita, “Evaluation of Water Suitability for Sustainable Seaweed (Kappaphycus Alvarezii) Cultivation to Support Science Technopark in North Kalimantan,” J. Pengelolaan Sumberd. Alam dan Lingkung. (Journal Nat. Resour. Environ. Manag., vol. 11, no. 3, pp. 490–503, Jan. 2022, doi: 10.29244/jpsl.11.3.490-503.

# Import Data

## 1. Pendahuluan
Tahap pertama dalam proses pengembangan model machine learning adalah mengimpor dataset yang akan digunakan. Dataset ini harus mencakup fitur-fitur yang relevan untuk masalah yang akan diselesaikan, dalam hal ini adalah prediksi kadar air berdasarkan intensitas cahaya dari sensor AS7265X. Data ini dapat berasal dari file *.xlsx.

## 2. Metode Import Data
Dataset diimpor ke dalam Python menggunakan pustaka **pandas**, yang memungkinkan manipulasi data dengan efisien. Format data yang digunakan adalah **XLSX**. Dataset ini dicetak dengan fungsi data.head() untuk melihat dataset yang diambil.
"""

#Ambil data dari google drive
from google.colab import drive
import pandas as pd

# Mount Google Drive
drive.mount('/content/drive')

# Path to your Excel file in Google Drive
file_path = '/content/drive/My Drive/DATA RUMPUT LAUT.xlsx'  # Replace with your file path

# Read the Excel file into a pandas DataFrame
data = pd.read_excel(file_path, sheet_name='data')

# Now you can work with the 'data' DataFrame
# For example, to display the first 5 rows:
data.head()

"""#DATA UNDERSTANDING
Data yang digunakan dalam proyek ini terdiri dari dua komponen utama:
1. **Fitur Input (X)**: Intensitas reflektansi cahaya yang diukur pada 18 channel sensor AS7265X. Setiap channel mengukur tingkat reflektansi pada panjang gelombang tertentu, yang memberikan gambaran tentang karakteristik optik rumput laut pada spektrum yang berbeda.
2. **Target Output (y)**: Nilai kadar air rumput laut yang diukur pada saat yang sama dengan pengambilan data reflektansi. Nilai ini adalah target yang ingin diprediksi oleh model machine learning.

**Tahapan Data Understanding**:
- Preview Data: Menampilkan struktur dataset.
- Statistik Deskriptif: Rata-rata, standar deviasi, dll.
"""

#Data Understanding
import seaborn as sns
import matplotlib.pyplot as plt

# Informasi struktur data
print("\nDataset Info:")
print(data.info())

# Statistik deskriptif
print("\nDescriptive Statistics:")
print(data.describe())

"""**Tahapan Data Understanding**:
- Distribusi Data: Histogram untuk setiap kanal.
- Matriks Korelasi: Visualisasi hubungan antar kanal dan target.
"""

# Distribusi Data
# Histogram untuk setiap kanal
data.hist(bins=20, figsize=(16, 12), color='steelblue', edgecolor='black')
plt.suptitle("Distribusi Data pada Setiap Fitur")
plt.show()

# Analisis Korelasi
# Matriks korelasi
corr_matrix = data.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title("Heatmap Korelasi")
plt.show()

"""**Tahapan Data Understanding**:
- Outliers: Deteksi menggunakan boxplot.
"""

# Deteksi Outliers
# Boxplot untuk masing-masing kanal
plt.figure(figsize=(16, 8))
sns.boxplot(data=data, orient="h", palette="Set2")
plt.title("Deteksi Outliers pada Setiap Fitur")
plt.show()

"""**Tahapan Data Understanding**:
- Scatter Plot: Hubungan antar kanal reflektansi dan nilai R².
- Distribusi Target: Distribusi kadar air.
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import linregress

# Scatter Plot untuk Hubungan Antar Kanal dan Kadar Air
plt.figure(figsize=(20, 20))  # Atur ukuran plot untuk semua kanal
plt.suptitle("Scatter Plot: Hubungan Setiap Kanal dan Kadar Air dengan R²", fontsize=20)

# Iterasi untuk setiap kanal (asumsi nama kolom Kanal_A, Kanal_B, ..., Kanal_L)
channels = ['Kanal A', 'Kanal B', 'Kanal C', 'Kanal D', 'Kanal E', 'Kanal F',
            'Kanal G', 'Kanal H', 'Kanal R', 'Kanal I', 'Kanal S', 'Kanal J',
            'Kanal T', 'Kanal U', 'Kanal V', 'Kanal W', 'Kanal K', 'Kanal L']

kanal = ['CH1','CH2','CH3','CH4','CH5','CH6','CH7','CH8','CH9','CH10','CH11','CH12','CH13',
         'CH14','CH15','CH16','CH17','CH18']

for i, (kanal_name, channel) in enumerate(zip(kanal, channels), 1):  # Iterasi menggunakan kanal untuk data
    plt.subplot(5, 4, i)  # Atur subplot grid (5 baris x 4 kolom untuk 18 kanal)

    # Ambil data kanal dan kadar air
    x = data[kanal_name]
    y = data['Kadar Air']

    # Lakukan regresi linier
    slope, intercept, r_value, p_value, std_err = linregress(x, y)

    # Hitung R²
    r_squared = r_value**2

    # Buat scatter plot
    plt.scatter(x, y, alpha=0.6, color='steelblue')

    # Tambahkan garis regresi
    plt.plot(x, slope*x + intercept, color='red', linestyle='--')

    # Tampilkan R² di plot
    plt.title(f"{channel} vs Kadar Air\nR² = {r_squared:.4f}")
    plt.xlabel(kanal_name)  # Label X sesuai nama kanal
    plt.ylabel("Kadar Air")  # Label Y tetap "Kadar Air"

plt.tight_layout(rect=[0, 0, 1, 0.95])  # Supaya tidak tumpang tindih
plt.show()

"""**Tahapan Data Understanding**:
- Distribusi Target: Distribusi kadar air.
"""

# Analisis Target
# Histogram target (misal: 'Kadar_Air')
plt.figure(figsize=(8, 6))
sns.histplot(data['Kadar Air'], kde=True, color='blue', bins=20)
plt.title("Distribusi Target: Kadar Air")
plt.xlabel("Kadar Air")
plt.ylabel("Frekuensi")
plt.show()

"""#**DATA PREPARATION**
##**1. Standarisasi**
Data fitur (selain kolom target Kadar Air) distandarisasi menggunakan StandardScaler agar setiap fitur memiliki mean 0 dan standar deviasi 1. Hal ini penting untuk memastikan PCA tidak bias terhadap fitur dengan skala besar.
##**2. PCA**
PCA diterapkan pada data yang telah distandarisasi untuk mengidentifikasi komponen utama yang paling signifikan dalam menjelaskan varians data.
##**3. Analisis Varians**
Varians yang dijelaskan oleh setiap komponen PCA dihitung menggunakan `explained_variance_ratio_`.Varians kumulatif dihitung untuk menentukan jumlah komponen yang cukup untuk menjelaskan sebagian besar informasi dataset.
##**4. Tabel Varians**
Sebuah tabel menampilkan varians yang dijelaskan dan varians kumulatif untuk setiap komponen, memberikan gambaran numerik terhadap kontribusi masing-masing komponen.

##**5. Visualisasi Data**
- Explained Variance: Grafik batang menunjukkan kontribusi masing-masing komponen PCA dalam menjelaskan varians dataset.
- Cumulative Variance: Grafik garis menunjukkan akumulasi varians yang dijelaskan, memudahkan identifikasi jumlah komponen optimal.

Output visualisasi ini membantu menentukan berapa banyak komponen PCA yang diperlukan untuk menjaga sebagian besar informasi dataset sambil mengurangi dimensi.
"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Standarisasi data
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data.drop(columns=['Kadar Air']))  # Hapus 'Kadar Air' dari data fitur

# Inisialisasi PCA dan fit pada data
pca = PCA()
pca.fit(data_scaled)

# Menyusun data explained variance dan cumulative variance
explained_variance = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance)

# Membuat DataFrame untuk menampilkan dalam bentuk tabel
variance_df = pd.DataFrame({
    'Explained Variance': explained_variance,
    'Cumulative Variance': cumulative_variance
}, index=[f'Komponen {i+1}' for i in range(len(explained_variance))])

# Menampilkan tabel
print(variance_df)

# Plot explained variance ratio
plt.figure(figsize=(8, 6))
plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)
plt.xlabel('PCA Komponen')
plt.ylabel('Varians yang Dijelaskan')
plt.title('Explained Variance untuk Setiap Komponen PCA')
plt.xticks(np.arange(1, len(pca.explained_variance_ratio_) + 1, 1))
plt.show()

# Menampilkan total explained variance kumulatif
explained_variance_cumulative = np.cumsum(pca.explained_variance_ratio_)
plt.figure(figsize=(8, 6))
plt.plot(range(1, len(explained_variance_cumulative) + 1), explained_variance_cumulative, marker='o', color='b')
plt.xlabel('Jumlah Komponen PCA')
plt.ylabel('Kumulatif Varians yang Dijelaskan')
plt.title('Kumulatif Varians yang Dijelaskan oleh PCA')
plt.xticks(np.arange(1, len(pca.explained_variance_ratio_) + 1, 1))
plt.grid(True)
plt.show()

"""Kode berikut ini digunakan untuk menganalisis loading dari setiap fitur terhadap komponen utama hasil PCA:
##**6. Ekstraksi Koefisien Loading**

Koefisien loading setiap fitur terhadap komponen utama diambil dari properti pca.components_. Loading ini menunjukkan kontribusi relatif fitur terhadap masing-masing komponen.

##**7. Penyusunan DataFrame**

Koefisien loading disusun dalam bentuk tabel (DataFrame), dengan kolom mewakili fitur asli dan baris mewakili komponen PCA, untuk memudahkan interpretasi.

##**8. Tampilan Loading Komponen Awal**

Hasil loading untuk 6 komponen pertama ditampilkan untuk memberikan wawasan tentang fitur mana yang memiliki kontribusi terbesar terhadap komponen utama.
"""

# Menampilkan koefisien loading untuk setiap komponen
pca_components = pca.components_

# Membuat DataFrame untuk memudahkan interpretasi
loading_df = pd.DataFrame(pca_components, columns=data.drop(columns=['Kadar Air']).columns)

# Menampilkan loading untuk 6 komponen pertama
print("Loading untuk 6 komponen pertama:")
print(loading_df.iloc[:6, :])

"""##**9. Standarisasi Data**
Fitur data distandarisasi menggunakan StandardScaler agar setiap fitur memiliki rata-rata 0 dan standar deviasi 1. Ini penting untuk PCA, yang sensitif terhadap skala data.

##**10. Penerapan PCA**
PCA dengan 6 komponen utama diinisialisasi dan di-fit pada data yang sudah distandarisasi. Komponen utama adalah kombinasi linier dari fitur-fitur asli.

##**11. Heatmap Komponen PCA**
Koefisien kontribusi fitur terhadap masing-masing komponen PCA divisualisasikan dengan heatmap.

Baris merepresentasikan komponen utama (PC1 hingga PC6).
Kolom merepresentasikan kanal (fitur asli).
Nilai menunjukkan kontribusi fitur terhadap komponen, dengan pewarnaan untuk memudahkan interpretasi.

##**12. Explained Variance**
Proporsi varians yang dijelaskan oleh setiap komponen utama dihitung (explained_variance_ratio_) dan ditampilkan untuk menunjukkan kontribusi masing-masing komponen terhadap total informasi dalam data.
"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Standarisasi data
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data.drop(columns=['Kadar Air']))  # Hapus 'Kadar Air' dari data fitur

# Inisialisasi PCA dan pilih 6 komponen utama
pca = PCA(n_components=6)
pca.fit(data_scaled)

# Mengambil komponen utama (koefisien) untuk 6 komponen pertama
pca_components = pca.components_

# Visualisasikan komponen PCA dengan heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(pca_components, annot=True, cmap='coolwarm', xticklabels=data.drop(columns=['Kadar Air']).columns,
            yticklabels=[f'PC{i+1}' for i in range(6)], cbar_kws={'label': 'Kontribusi Komponen'}, fmt='.2f')

plt.title('Heatmap Kontribusi Setiap Kanal terhadap 6 Komponen PCA')
plt.xlabel('Kanal')
plt.ylabel('Komponen PCA')
plt.show()

# Menampilkan variance yang dijelaskan oleh masing-masing komponen
explained_variance = pca.explained_variance_ratio_
print(f"Explained Variance per Komponen: {explained_variance}")

"""## **3. Train-Test-Split**
1. **Menyiapkan Data**: Data `X` berisi fitur-fitur hasil PCA, dan data `Y` berisi nilai kadar air.
2. **Melakukan Split**: Fungsi `train_test_split` digunakan untuk membagi data secara acak dengan proporsi yang ditentukan.
3. **Output**: Hasil pembagian adalah empat dataset:
   - `X_train`: Data pelatihan untuk fitur.
   - `X_test`: Data pengujian untuk fitur.
   - `Y_train`: Data pelatihan untuk target kadar air.
   - `Y_test`: Data pengujian untuk target kadar air.
"""

from sklearn.model_selection import train_test_split

# Misalkan data_pca adalah data yang telah melalui PCA (X)
# Misalkan target adalah kadar air (Y)
# Transformasi data dengan PCA
data_pca = pca.transform(data_scaled)  # Transformasikan data menggunakan model PCA
X = data_pca  # Data komponen PCA
Y = data['Kadar Air']  # Target kadar air

# Melakukan train-test split dengan proporsi 80% untuk training dan 20% untuk testing
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Menampilkan ukuran data setelah split
print(f'Ukuran data pelatihan X: {X_train.shape}')
print(f'Ukuran data pengujian X: {X_test.shape}')
print(f'Ukuran data pelatihan Y: {Y_train.shape}')
print(f'Ukuran data pengujian Y: {Y_test.shape}')

"""## Modeling
### 1. **Random Forest**
   - **Parameter yang digunakan**:

`n_estimators=100`: Model membangun 100 pohon keputusan.

`random_state=42`: Parameter ini memastikan replikasi hasil dengan menetapkan nilai acak yang sama.

### 2. **Neural Network (MLP)**

   - **Parameter yang digunakan**:

`hidden_layer_sizes=(50, 30)`: Model terdiri dari dua lapisan tersembunyi, masing-masing memiliki 50 dan 30 neuron.

`max_iter=1000`: Model dilatih hingga maksimal 1.000 iterasi untuk konvergensi.

`random_state=42`: Memastikan hasil yang dapat direproduksi.

### 3. **K-Nearest Neighbors (KNN)**
   - **Parameter utama:** `n_neighbors=5`: Model menggunakan 5 tetangga terdekat dalam perhitungannya.

### 4. **AdaBoost (Adaptive Boosting)**
   - **Parameter yang digunakan:**

`n_estimators=50`: Model menjalankan 50 iterasi untuk menguatkan prediksi.

`random_state=42`: Menetapkan nilai awal acak untuk hasil yang dapat direplikasi.

### 5. **Gradient Boosting**
   - **Parameter yang digunakan:**

`n_estimators=100`: Model membangun 100 pohon keputusan untuk memaksimalkan akurasi.

`random_state=42`: Memastikan hasil yang konsisten.

### 6. **Deep Neural Network (DNN)**
   - **Parameter yang digunakan:**

Loss function: `mean_squared_error` digunakan untuk mengukur kesalahan regresi.

Optimizer: `adam` untuk pembaruan bobot yang efisien.

Epochs: Model dilatih selama `100 epoch`.

Batch size: `32 data` diproses dalam setiap iterasi pembaruan bobot.


### 7. **Decision Tree**
   - **Parameter yang digunakan:** `random_state=42`: Menetapkan nilai acak yang memastikan konsistensi hasil.
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input


# 1. RANDOM FOREST
print("RANDOM FOREST")
# Inisialisasi model Random Forest
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Melatih model dengan data pelatihan
rf_model.fit(X_train, Y_train)

# 2. MLP (multi-layer perceptron)
print("MLP")
# Inisialisasi model Neural Network (MLP)
nn_model = MLPRegressor(hidden_layer_sizes=(50, 30), max_iter=1000, random_state=42)

# Melatih model dengan data pelatihan
nn_model.fit(X_train, Y_train)

# 3. KNN
print("KNN")
# Inisialisasi model KNN
knn_model = KNeighborsRegressor(n_neighbors=5)

# Melatih model dengan data pelatihan
knn_model.fit(X_train, Y_train)

# 4. ADABOOST
print("ADABOOST")
# Inisialisasi model AdaBoost
ada_model = AdaBoostRegressor(n_estimators=50, random_state=42)

# Melatih model dengan data pelatihan
ada_model.fit(X_train, Y_train)

# 5. GRADIENT BOOST
print("GRADIENT BOOST")
# Inisialisasi model Gradient Boosting
gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)

# Melatih model dengan data pelatihan
gb_model.fit(X_train, Y_train)

# 6. Model Deep Learning Neural Network
print("DNN")
# Inisialisasi model Deep Learning (DNN)
model_nn = Sequential()
model_nn.add(Input(shape=(6,)))  # Sesuaikan input dengan jumlah fitur yang benar
model_nn.add(Dense(64, input_dim=18, activation='relu'))  # Layer pertama dengan 64 neuron
model_nn.add(Dense(32, activation='relu'))  # Layer kedua dengan 32 neuron
model_nn.add(Dense(1))  # Output layer untuk regresi

# Kompilasi model
model_nn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])

# Melatih model dengan data pelatihan
history_nn = model_nn.fit(X_train, Y_train, epochs=100, batch_size=32, validation_data=(X_test, Y_test), verbose=1)

# 7. Model Decision Tree
print("DECISION TREE")
# Inisialisasi model DecisionTreeRegressor
dt_model = DecisionTreeRegressor(random_state=42)

# Melatih model dengan data pelatihan
dt_model.fit(X_train, Y_train)

"""**Evaluasi Metrik**

1. MSE : Mean Square Error
2. MAE : Mean Absolute Error
3. RMSE: Root Mean Square Error
4. R² : Koefisien Determinasi
"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Prediksi menggunakan data pelatihan dan data pengujian
Y_pred_train_rf = rf_model.predict(X_train)
Y_pred_test_rf = rf_model.predict(X_test)

Y_pred_train_nn = nn_model.predict(X_train)
Y_pred_test_nn = nn_model.predict(X_test)

Y_pred_train_knn = knn_model.predict(X_train)
Y_pred_test_knn = knn_model.predict(X_test)

Y_pred_train_ada = ada_model.predict(X_train)
Y_pred_test_ada = ada_model.predict(X_test)

Y_pred_train_gb = gb_model.predict(X_train)
Y_pred_test_gb = gb_model.predict(X_test)

Y_pred_train_dnn = model_nn.predict(X_train)
Y_pred_test_dnn = model_nn.predict(X_test)

Y_pred_train_dt = dt_model.predict(X_train)
Y_pred_test_dt = dt_model.predict(X_test)

# Menghitung MSE, MAE, RMSE, dan R^2 untuk Random Forest (training & testing)
mse_train_rf = mean_squared_error(Y_train, Y_pred_train_rf)
mae_train_rf = mean_absolute_error(Y_train, Y_pred_train_rf)
rmse_train_rf = np.sqrt(mse_train_rf)
r2_train_rf = r2_score(Y_train, Y_pred_train_rf)

mse_test_rf = mean_squared_error(Y_test, Y_pred_test_rf)
mae_test_rf = mean_absolute_error(Y_test, Y_pred_test_rf)
rmse_test_rf = np.sqrt(mse_test_rf)
r2_test_rf = r2_score(Y_test, Y_pred_test_rf)

# Menghitung MSE, MAE, RMSE, dan R^2 untuk Neural Network (training & testing)
mse_train_nn = mean_squared_error(Y_train, Y_pred_train_nn)
mae_train_nn = mean_absolute_error(Y_train, Y_pred_train_nn)
rmse_train_nn = np.sqrt(mse_train_nn)
r2_train_nn = r2_score(Y_train, Y_pred_train_nn)

mse_test_nn = mean_squared_error(Y_test, Y_pred_test_nn)
mae_test_nn = mean_absolute_error(Y_test, Y_pred_test_nn)
rmse_test_nn = np.sqrt(mse_test_nn)
r2_test_nn = r2_score(Y_test, Y_pred_test_nn)

# Menghitung MSE, MAE, RMSE, dan R^2 untuk KNN (training & testing)
mse_train_knn = mean_squared_error(Y_train, Y_pred_train_knn)
mae_train_knn = mean_absolute_error(Y_train, Y_pred_train_knn)
rmse_train_knn = np.sqrt(mse_train_knn)
r2_train_knn = r2_score(Y_train, Y_pred_train_knn)

mse_test_knn = mean_squared_error(Y_test, Y_pred_test_knn)
mae_test_knn = mean_absolute_error(Y_test, Y_pred_test_knn)
rmse_test_knn = np.sqrt(mse_test_knn)
r2_test_knn = r2_score(Y_test, Y_pred_test_knn)

# Menghitung MSE, MAE, RMSE, dan R^2 untuk ADABOOST (training & testing)
mse_train_ada = mean_squared_error(Y_train, Y_pred_train_ada)
mae_train_ada = mean_absolute_error(Y_train, Y_pred_train_ada)
rmse_train_ada = np.sqrt(mse_train_ada)
r2_train_ada = r2_score(Y_train, Y_pred_train_ada)

mse_test_ada = mean_squared_error(Y_test, Y_pred_test_ada)
mae_test_ada = mean_absolute_error(Y_test, Y_pred_test_ada)
rmse_test_ada = np.sqrt(mse_test_ada)
r2_test_ada = r2_score(Y_test, Y_pred_test_ada)

# Menghitung MSE, MAE, RMSE, dan R^2 untuk Gradient Boost (training & testing)
mse_train_gb = mean_squared_error(Y_train, Y_pred_train_gb)
mae_train_gb = mean_absolute_error(Y_train, Y_pred_train_gb)
rmse_train_gb = np.sqrt(mse_train_gb)
r2_train_gb = r2_score(Y_train, Y_pred_train_gb)

mse_test_gb = mean_squared_error(Y_test, Y_pred_test_gb)
mae_test_gb = mean_absolute_error(Y_test, Y_pred_test_gb)
rmse_test_gb = np.sqrt(mse_test_gb)
r2_test_gb = r2_score(Y_test, Y_pred_test_gb)

# Menghitung MSE, MAE, RMSE, dan R^2 untuk Deep Neural Network (training & testing)
mse_train_dnn = mean_squared_error(Y_train, Y_pred_train_dnn)
mae_train_dnn = mean_absolute_error(Y_train, Y_pred_train_dnn)
rmse_train_dnn = np.sqrt(mse_train_dnn)
r2_train_dnn = r2_score(Y_train, Y_pred_train_dnn)

mse_test_dnn = mean_squared_error(Y_test, Y_pred_test_dnn)
mae_test_dnn = mean_absolute_error(Y_test, Y_pred_test_dnn)
rmse_test_dnn = np.sqrt(mse_test_dnn)
r2_test_dnn = r2_score(Y_test, Y_pred_test_dnn)

# Menghitung MSE, MAE, RMSE, dan R^2 untuk Decision Tree (training & testing)
mse_train_dt = mean_squared_error(Y_train, Y_pred_train_dt)
mae_train_dt = mean_absolute_error(Y_train, Y_pred_train_dt)
rmse_train_dt = np.sqrt(mse_train_dt)
r2_train_dt = r2_score(Y_train, Y_pred_train_dt)

mse_test_dt = mean_squared_error(Y_test, Y_pred_test_dt)
mae_test_dt = mean_absolute_error(Y_test, Y_pred_test_dt)
rmse_test_dt = np.sqrt(mse_test_dt)
r2_test_dt = r2_score(Y_test, Y_pred_test_dt)

# Tampilkan evaluasi
print("MODEL EVALUATION")
models = ['Random Forest', 'Neural Network', 'KNN', 'Adaboost', 'Gradient Boost', 'Deep Neural Network', 'Decision Tree']
metrics = ['MSE (Train)', 'MAE (Train)', 'RMSE (Train)', 'R² (Train)', 'MSE (Test)', 'MAE (Test)', 'RMSE (Test)', 'R² (Test)']

# Hasil evaluasi
results = [
    [mse_train_rf, mae_train_rf, rmse_train_rf, r2_train_rf, mse_test_rf, mae_test_rf, rmse_test_rf, r2_test_rf],
    [mse_train_nn, mae_train_nn, rmse_train_nn, r2_train_nn, mse_test_nn, mae_test_nn, rmse_test_nn, r2_test_nn],
    [mse_train_knn, mae_train_knn, rmse_train_knn, r2_train_knn, mse_test_knn, mae_test_knn, rmse_test_knn, r2_test_knn],
    [mse_train_ada, mae_train_ada, rmse_train_ada, r2_train_ada, mse_test_ada, mae_test_ada, rmse_test_ada, r2_test_ada],
    [mse_train_gb, mae_train_gb, rmse_train_gb, r2_train_gb, mse_test_gb, mae_test_gb, rmse_test_gb, r2_test_gb],
    [mse_train_dnn, mae_train_dnn, rmse_train_dnn, r2_train_dnn, mse_test_dnn, mae_test_dnn, rmse_test_dnn, r2_test_dnn],
    [mse_train_dt, mae_train_dt, rmse_train_dt, r2_train_dt, mse_test_dt, mae_test_dt, rmse_test_dt, r2_test_dt]
]

# Print as table
eval_df = pd.DataFrame(results, columns=metrics, index=models)
print(eval_df)

"""plotting hasil evaluasi"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Misalkan eval_df adalah DataFrame yang berisi hasil evaluasi model yang telah dihitung sebelumnya
# Anda perlu memastikan bahwa hasil evaluasi untuk train dan test sudah ada di eval_df

# Membuat plotting untuk MAE, MSE, RMSE, dan R²

metrics = ['MAE', 'MSE', 'RMSE', 'R²']
models = eval_df.index  # Model names
train_metrics = ['MAE (Train)', 'MSE (Train)', 'RMSE (Train)', 'R² (Train)']
test_metrics = ['MAE (Test)', 'MSE (Test)', 'RMSE (Test)', 'R² (Test)']

# Set plot size
fig, axes = plt.subplots(4, 1, figsize=(12, 10))

# Plot untuk MAE
axes[0].bar(models, eval_df[train_metrics[0]], width=0.3, label='Train', color='c', align='center')
axes[0].bar(models, eval_df[test_metrics[0]], width=0.3, label='Test', color='m', align='edge')
axes[0].set_title('MAE Comparison')
axes[0].set_ylabel('MAE')
axes[0].legend()

# Plot untuk MSE
axes[1].bar(models, eval_df[train_metrics[1]], width=0.3, label='Train', color='c', align='center')
axes[1].bar(models, eval_df[test_metrics[1]], width=0.3, label='Test', color='m', align='edge')
axes[1].set_title('MSE Comparison')
axes[1].set_ylabel('MSE')
axes[1].legend()

# Plot untuk RMSE
axes[2].bar(models, eval_df[train_metrics[2]], width=0.3, label='Train', color='c', align='center')
axes[2].bar(models, eval_df[test_metrics[2]], width=0.3, label='Test', color='m', align='edge')
axes[2].set_title('RMSE Comparison')
axes[2].set_ylabel('RMSE')
axes[2].legend()

# Plot untuk R²
axes[3].bar(models, eval_df[train_metrics[3]], width=0.3, label='Train', color='c', align='center')
axes[3].bar(models, eval_df[test_metrics[3]], width=0.3, label='Test', color='m', align='edge')
axes[3].set_title('R² Comparison')
axes[3].set_ylabel('R²')
axes[3].legend()

# Adjust layout and show plot
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error

# Ambil 10 data acak dari data training (X_train dan Y_train) menggunakan numpy
random_indices = np.random.choice(len(X_train), size=10, replace=False)

# Memastikan pengambilan data untuk keduanya X_train dan Y_train menggunakan iloc atau array indexing
X_random = X_train[random_indices]
Y_random = Y_train.iloc[random_indices]  # Pastikan menggunakan iloc untuk Series (Y_train)

# Prediksi kadar air menggunakan model-model yang telah dibangun
Y_pred_rf = rf_model.predict(X_random)
Y_pred_nn = nn_model.predict(X_random)
Y_pred_knn = knn_model.predict(X_random)
Y_pred_ada = ada_model.predict(X_random)
Y_pred_gb = gb_model.predict(X_random)
Y_pred_dnn = model_nn.predict(X_random)
Y_pred_dt = dt_model.predict(X_random)

# Menghitung error (MAE) untuk setiap model
mae_rf = mean_absolute_error(Y_random, Y_pred_rf)
mae_nn = mean_absolute_error(Y_random, Y_pred_nn)
mae_knn = mean_absolute_error(Y_random, Y_pred_knn)
mae_ada = mean_absolute_error(Y_random, Y_pred_ada)
mae_gb = mean_absolute_error(Y_random, Y_pred_gb)
mae_dnn = mean_absolute_error(Y_random, Y_pred_dnn)
mae_dt = mean_absolute_error(Y_random, Y_pred_dt)

# Menyusun hasil prediksi dalam dictionary (memastikan format 1 dimensi untuk setiap model)
results = {
    'Actual Value': np.round(Y_random.values, 2),  # Round to 2 decimal places
    'RF Prediction': np.round(Y_pred_rf.flatten(), 2),  # Round to 2 decimal places
    'NN Prediction': np.round(Y_pred_nn.flatten(), 2),  # Round to 2 decimal places
    'KNN Prediction': np.round(Y_pred_knn.flatten(), 2),  # Round to 2 decimal places
    'Adaboost Prediction': np.round(Y_pred_ada.flatten(), 2),  # Round to 2 decimal places
    'GB Prediction': np.round(Y_pred_gb.flatten(), 2),  # Round to 2 decimal places
    'DNN Prediction': np.round(Y_pred_dnn.flatten(), 2),  # Round to 2 decimal places
    'DT Prediction': np.round(Y_pred_dt.flatten(), 2)   # Round to 2 decimal places
}

# Convert to DataFrame
eval_df = pd.DataFrame(results)

# Menambahkan kolom MAE per model ke DataFrame dan membulatkan nilai MAE ke 2 angka desimal
eval_df['RF Error (MAE)'] = np.round(mae_rf, 4)
eval_df['NN Error (MAE)'] = np.round(mae_nn, 4)
eval_df['KNN Error (MAE)'] = np.round(mae_knn, 4)
eval_df['Adaboost Error (MAE)'] = np.round(mae_ada, 4)
eval_df['GB Error (MAE)'] = np.round(mae_gb, 4)
eval_df['DNN Error (MAE)'] = np.round(mae_dnn, 4)
eval_df['DT Error (MAE)'] = np.round(mae_dt, 4)

# Menampilkan DataFrame hasil evaluasi
print(eval_df)